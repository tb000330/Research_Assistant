{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"교차검증.ipynb","provenance":[],"collapsed_sections":["Pyk6tqkI9ZY4"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EjfLvOsd0Zm1","executionInfo":{"status":"ok","timestamp":1654511282195,"user_tz":-540,"elapsed":19918,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"07b29608-bcf9-4a95-e843-5de53ab8c564"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["import numpy as np #대수학\n","import pandas as pd #전처리\n","\n","import seaborn as sns #시각화\n","from matplotlib.patches import Patch\n","from matplotlib import pyplot as plt\n","\n","from tensorflow.keras.models import load_model\n","\n","\n","plt.rcParams.update({'figure.max_open_warning': 0})\n","plt.style.use('fivethirtyeight')\n","cmap_data = plt.cm.Paired\n","cmap_cv = plt.cm.coolwarm\n","\n","import warnings\n","%matplotlib inline\n","\n","import configparser\n","from sklearn.preprocessing import MinMaxScaler\n","\n","import re\n","import time\n","from tqdm import tqdm\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import datetime as dt\n","\n","import os # 경로\n","\n","import numpy as np\n","import sys\n","import random \n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.layers import SimpleRNN, LSTM, Bidirectional\n","\n","import configparser\n","from keras.models import Sequential\n","from keras.layers import Dense, Embedding\n","from keras.layers import SimpleRNN\n","from keras.layers import Dropout\n","from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler"],"metadata":{"id":"qqwM6Qd90aHt","executionInfo":{"status":"ok","timestamp":1654511285567,"user_tz":-540,"elapsed":3384,"user":{"displayName":"김다은","userId":"16190703000834159129"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["### covid 포함"],"metadata":{"id":"8iRpypnp9PSd"}},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/gdp_re/training_data_0524.csv')\n","data = data.drop(['Unnamed: 0'], axis=1)\n","data.tail()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"yL6gsaTp5ax6","executionInfo":{"status":"ok","timestamp":1654511286287,"user_tz":-540,"elapsed":730,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"a748bcce-a8bc-47a1-db9f-6d6c70f13572"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Y   Y1   Y2   Y3              YY1                    YY2  \\\n","72 -1.3 -3.2  2.2  1.1  ['-1.3' '-3.2']  ['-1.3' '-3.2' '2.2']   \n","73 -3.2  2.2  1.1  1.7   ['-3.2' '2.2']   ['-3.2' '2.2' '1.1']   \n","74  2.2  1.1  1.7  0.8    ['2.2' '1.1']    ['2.2' '1.1' '1.7']   \n","75  1.1  1.7  0.8  0.3    ['1.1' '1.7']    ['1.1' '1.7' '0.8']   \n","76  1.7  0.8  0.3  1.2    ['1.7' '0.8']    ['1.7' '0.8' '0.3']   \n","\n","                            YY3  MONTH  YEAR     QRT  ...   X16_L0   X16_L1  \\\n","72  ['-1.3' '-3.2' '2.2' '1.1']      1  2020  2020Q1  ...  2119.01  2197.67   \n","73   ['-3.2' '2.2' '1.1' '1.7']      4  2020  2020Q2  ...  1947.56  1754.64   \n","74    ['2.2' '1.1' '1.7' '0.8']      7  2020  2020Q3  ...  2249.37  2108.33   \n","75    ['1.1' '1.7' '0.8' '0.3']     10  2020  2020Q4  ...  2267.15  2327.89   \n","76    ['1.7' '0.8' '0.3' '1.2']      1  2021  2021Q1  ...  2976.21  2873.47   \n","\n","     X16_L2   X16_L3  X17_L1  X17_L2  X17_L3  X18_L1  X18_L2  X18_L3  \n","72  2087.96  2083.48  109.65  106.59  110.77  120.52  112.14  115.69  \n","73  1987.01  2119.01  110.41   97.41   99.07  116.45  102.29  118.39  \n","74  2029.60  1947.56   92.85   82.16   86.99   99.11   95.86  105.13  \n","75  2326.17  2249.37  113.80   94.52  100.72  107.69   98.61  107.70  \n","76  2591.34  2267.15  120.21  110.69  106.95  123.99  111.52  108.07  \n","\n","[5 rows x 60 columns]"],"text/html":["\n","  <div id=\"df-590f7fc4-5c50-489e-b425-34f018fbd346\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Y</th>\n","      <th>Y1</th>\n","      <th>Y2</th>\n","      <th>Y3</th>\n","      <th>YY1</th>\n","      <th>YY2</th>\n","      <th>YY3</th>\n","      <th>MONTH</th>\n","      <th>YEAR</th>\n","      <th>QRT</th>\n","      <th>...</th>\n","      <th>X16_L0</th>\n","      <th>X16_L1</th>\n","      <th>X16_L2</th>\n","      <th>X16_L3</th>\n","      <th>X17_L1</th>\n","      <th>X17_L2</th>\n","      <th>X17_L3</th>\n","      <th>X18_L1</th>\n","      <th>X18_L2</th>\n","      <th>X18_L3</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>72</th>\n","      <td>-1.3</td>\n","      <td>-3.2</td>\n","      <td>2.2</td>\n","      <td>1.1</td>\n","      <td>['-1.3' '-3.2']</td>\n","      <td>['-1.3' '-3.2' '2.2']</td>\n","      <td>['-1.3' '-3.2' '2.2' '1.1']</td>\n","      <td>1</td>\n","      <td>2020</td>\n","      <td>2020Q1</td>\n","      <td>...</td>\n","      <td>2119.01</td>\n","      <td>2197.67</td>\n","      <td>2087.96</td>\n","      <td>2083.48</td>\n","      <td>109.65</td>\n","      <td>106.59</td>\n","      <td>110.77</td>\n","      <td>120.52</td>\n","      <td>112.14</td>\n","      <td>115.69</td>\n","    </tr>\n","    <tr>\n","      <th>73</th>\n","      <td>-3.2</td>\n","      <td>2.2</td>\n","      <td>1.1</td>\n","      <td>1.7</td>\n","      <td>['-3.2' '2.2']</td>\n","      <td>['-3.2' '2.2' '1.1']</td>\n","      <td>['-3.2' '2.2' '1.1' '1.7']</td>\n","      <td>4</td>\n","      <td>2020</td>\n","      <td>2020Q2</td>\n","      <td>...</td>\n","      <td>1947.56</td>\n","      <td>1754.64</td>\n","      <td>1987.01</td>\n","      <td>2119.01</td>\n","      <td>110.41</td>\n","      <td>97.41</td>\n","      <td>99.07</td>\n","      <td>116.45</td>\n","      <td>102.29</td>\n","      <td>118.39</td>\n","    </tr>\n","    <tr>\n","      <th>74</th>\n","      <td>2.2</td>\n","      <td>1.1</td>\n","      <td>1.7</td>\n","      <td>0.8</td>\n","      <td>['2.2' '1.1']</td>\n","      <td>['2.2' '1.1' '1.7']</td>\n","      <td>['2.2' '1.1' '1.7' '0.8']</td>\n","      <td>7</td>\n","      <td>2020</td>\n","      <td>2020Q3</td>\n","      <td>...</td>\n","      <td>2249.37</td>\n","      <td>2108.33</td>\n","      <td>2029.60</td>\n","      <td>1947.56</td>\n","      <td>92.85</td>\n","      <td>82.16</td>\n","      <td>86.99</td>\n","      <td>99.11</td>\n","      <td>95.86</td>\n","      <td>105.13</td>\n","    </tr>\n","    <tr>\n","      <th>75</th>\n","      <td>1.1</td>\n","      <td>1.7</td>\n","      <td>0.8</td>\n","      <td>0.3</td>\n","      <td>['1.1' '1.7']</td>\n","      <td>['1.1' '1.7' '0.8']</td>\n","      <td>['1.1' '1.7' '0.8' '0.3']</td>\n","      <td>10</td>\n","      <td>2020</td>\n","      <td>2020Q4</td>\n","      <td>...</td>\n","      <td>2267.15</td>\n","      <td>2327.89</td>\n","      <td>2326.17</td>\n","      <td>2249.37</td>\n","      <td>113.80</td>\n","      <td>94.52</td>\n","      <td>100.72</td>\n","      <td>107.69</td>\n","      <td>98.61</td>\n","      <td>107.70</td>\n","    </tr>\n","    <tr>\n","      <th>76</th>\n","      <td>1.7</td>\n","      <td>0.8</td>\n","      <td>0.3</td>\n","      <td>1.2</td>\n","      <td>['1.7' '0.8']</td>\n","      <td>['1.7' '0.8' '0.3']</td>\n","      <td>['1.7' '0.8' '0.3' '1.2']</td>\n","      <td>1</td>\n","      <td>2021</td>\n","      <td>2021Q1</td>\n","      <td>...</td>\n","      <td>2976.21</td>\n","      <td>2873.47</td>\n","      <td>2591.34</td>\n","      <td>2267.15</td>\n","      <td>120.21</td>\n","      <td>110.69</td>\n","      <td>106.95</td>\n","      <td>123.99</td>\n","      <td>111.52</td>\n","      <td>108.07</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 60 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-590f7fc4-5c50-489e-b425-34f018fbd346')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-590f7fc4-5c50-489e-b425-34f018fbd346 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-590f7fc4-5c50-489e-b425-34f018fbd346');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["data[\"Y\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bsGL_tIE1JQJ","executionInfo":{"status":"ok","timestamp":1654511286289,"user_tz":-540,"elapsed":40,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"00f69ff2-7edf-40ba-acfa-2651de59ec4f"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     2.5\n","1     1.8\n","2     2.0\n","3     1.1\n","4    -0.7\n","     ... \n","72   -1.3\n","73   -3.2\n","74    2.2\n","75    1.1\n","76    1.7\n","Name: Y, Length: 77, dtype: float64"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["data = data[data.iloc[:, 9:].apply(lambda row : sum(pd.isna(row)), axis = 1) == 0]\n","data = data.reset_index(drop = True)\n","data[\"Y\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HIZNkVw5QKS","executionInfo":{"status":"ok","timestamp":1654511286291,"user_tz":-540,"elapsed":35,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"f714a099-4f18-4b96-ded8-7daee743fff6"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0     2.5\n","1     1.8\n","2     2.0\n","3     1.1\n","4    -0.7\n","     ... \n","72   -1.3\n","73   -3.2\n","74    2.2\n","75    1.1\n","76    1.7\n","Name: Y, Length: 77, dtype: float64"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["data['TIME'] = data['TIME'].astype(str)\n","\n","time_list = []\n","for i in range(len(data)):\n","  time_i = data['TIME'][i]\n","  time = re.sub(r'(\\d{4})(\\d{2})', '\\g<1>-\\g<2>', time_i) #정규표현식 'YYYY-mm'\n","  time_list.append(time)\n","      \n","data['TIME'] = time_list\n","data['datetime'] = pd.to_datetime(data['TIME'],  format=\"%Y-%m\") #datetime변환\n","data.sort_values('datetime', ignore_index = True, inplace=True)\n","data.set_index(['YEAR'], inplace=True)"],"metadata":{"id":"MBgMMmjzrbHT","executionInfo":{"status":"ok","timestamp":1654511286292,"user_tz":-540,"elapsed":30,"user":{"displayName":"김다은","userId":"16190703000834159129"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["train = data.drop(['Y','Y1','Y2','Y3', 'YY1','YY2','YY3','MONTH','QRT','TIME','STATE', 'datetime'], axis=1)\n","train.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fsbcb-hUx5uR","executionInfo":{"status":"ok","timestamp":1654511286293,"user_tz":-540,"elapsed":28,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"056cbee6-b6f8-477b-86e3-1fc9af5cc634"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['X1_L2', 'X1_L3', 'X2_L2', 'X2_L3', 'X3_L2', 'X3_L3', 'X4_L2', 'X4_L3',\n","       'X5_L2', 'X5_L3', 'X6_L2', 'X6_L3', 'X7_L2', 'X7_L3', 'X8_L1', 'X8_L2',\n","       'X8_L3', 'X9_L1', 'X9_L2', 'X9_L3', 'X10_L1', 'X10_L2', 'X10_L3',\n","       'X11_L1', 'X11_L2', 'X11_L3', 'X12_L2', 'X12_L3', 'X13_L2', 'X13_L3',\n","       'X14_L0', 'X14_L1', 'X14_L2', 'X14_L3', 'X15_L0', 'X15_L1', 'X15_L2',\n","       'X15_L3', 'X16_L0', 'X16_L1', 'X16_L2', 'X16_L3', 'X17_L1', 'X17_L2',\n","       'X17_L3', 'X18_L1', 'X18_L2', 'X18_L3'],\n","      dtype='object')"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# train[\"X7_L2\"] = train[\"X7_L2\"] / 100000 # 10 ** 6\n","# train[\"X7_L3\"] = train[\"X7_L3\"] / 100000 # 10 ** 6"],"metadata":{"id":"LU8c2zHAsrl1","executionInfo":{"status":"ok","timestamp":1654511286295,"user_tz":-540,"elapsed":24,"user":{"displayName":"김다은","userId":"16190703000834159129"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# train[\"Y\"] = data[\"Y\"]"],"metadata":{"id":"CthzO-G6uSKo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# train[\"Y\"]"],"metadata":{"id":"gSDVjgayuY2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 표준화\n","from sklearn.preprocessing import StandardScaler\n","# train = train.drop(['MONTH', 'YEAR', 'QRT', 'TIME', 'STATE', 'Y1',\t'Y2',\t'Y3',\t'YY1',\t'YY2',\t'YY3', 'datetime'], axis=1)\n","# train = StandardScaler().fit_transform(train)\n","# pd.DataFrame(train)"],"metadata":{"id":"KDXUw7TF1VM5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 정규화\n","scaler = MinMaxScaler()\n","scaler.fit(train)\n","train = scaler.transform(train)\n","train"],"metadata":{"id":"h7khlcLZrbKD","executionInfo":{"status":"ok","timestamp":1654346843832,"user_tz":-540,"elapsed":4,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f83290d-be29-405c-b0f7-e06107e3d4e1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.10294118, 0.        , 0.58490566, ..., 0.        , 0.03074177,\n","        0.00414978],\n","       [0.77941176, 0.59210526, 0.67924528, ..., 0.021448  , 0.        ,\n","        0.        ],\n","       [1.        , 1.        , 0.8490566 , ..., 0.01977401, 0.06078937,\n","        0.03194364],\n","       ...,\n","       [0.10294118, 0.19736842, 0.        , ..., 0.71186441, 0.66283221,\n","        0.71125265],\n","       [0.25      , 0.23684211, 0.18867925, ..., 0.80163214, 0.69010313,\n","        0.73605482],\n","       [0.41176471, 0.36842105, 0.45283019, ..., 0.97216991, 0.81812773,\n","        0.73962555]])"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["df = pd.DataFrame(train, columns=['X1_L2', 'X1_L3', 'X2_L2', 'X2_L3', 'X3_L2', 'X3_L3', 'X4_L2',\n","       'X4_L3', 'X5_L2', 'X5_L3', 'X6_L2', 'X6_L3', 'X7_L2', 'X7_L3', 'X8_L1',\n","       'X8_L2', 'X8_L3', 'X9_L1', 'X9_L2', 'X9_L3', 'X10_L1', 'X10_L2',\n","       'X10_L3', 'X11_L1', 'X11_L2', 'X11_L3', 'X12_L2', 'X12_L3', 'X13_L2',\n","       'X13_L3', 'X14_L0', 'X14_L1', 'X14_L2', 'X14_L3', 'X15_L0', 'X15_L1',\n","       'X15_L2', 'X15_L3', 'X16_L0', 'X16_L1', 'X16_L2', 'X16_L3', 'X17_L1',\n","       'X17_L2', 'X17_L3', 'X18_L1', 'X18_L2', 'X18_L3'], index=data.index)\n","df[\"Y\"] = np.log1p(data[\"Y\"])"],"metadata":{"id":"c7cxJzMCw9Lk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":519},"id":"QQf_UjtnvzIx","executionInfo":{"status":"ok","timestamp":1654350069300,"user_tz":-540,"elapsed":281,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"6e9660de-708f-4b96-8d0b-e6629073a4ed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["         X1_L2     X1_L3     X2_L2  X2_L3     X3_L2     X3_L3     X4_L2  \\\n","YEAR                                                                      \n","2002  0.102941  0.000000  0.584906   0.48  0.119093  0.000000  0.546875   \n","2002  0.779412  0.592105  0.679245   0.54  0.000000  0.012397  0.692708   \n","2002  1.000000  1.000000  0.849057   0.86  0.177694  0.090909  0.776042   \n","2002  0.676471  0.736842  0.754717   0.74  0.119093  0.090909  0.854167   \n","2003  0.455882  0.486842  0.849057   0.78  0.206049  0.138430  0.723958   \n","...        ...       ...       ...    ...       ...       ...       ...   \n","2020  0.220588  0.223684  0.603774   0.60  1.000000  1.000000  0.468750   \n","2020  0.264706  0.302632  0.584906   0.68  0.809074  0.882231  0.328125   \n","2020  0.102941  0.197368  0.000000   0.08  0.848771  0.838843  0.000000   \n","2020  0.250000  0.236842  0.188679   0.00  0.839319  0.929752  0.359375   \n","2021  0.411765  0.368421  0.452830   0.30  0.986767  0.933884  0.510417   \n","\n","         X4_L3     X5_L2     X5_L3  ...    X16_L1    X16_L2    X16_L3  \\\n","YEAR                                ...                                 \n","2002  0.608040  0.111301  0.000000  ...  0.067586  0.033960  0.000000   \n","2002  0.653266  0.059932  0.028056  ...  0.153942  0.121315  0.103645   \n","2002  0.844221  0.176370  0.114228  ...  0.088554  0.109613  0.150115   \n","2002  0.783920  0.111301  0.080160  ...  0.047361  0.079850  0.088818   \n","2003  0.763819  0.188356  0.146293  ...  0.039290  0.074096  0.059700   \n","...        ...       ...       ...  ...       ...       ...       ...   \n","2020  0.562814  1.000000  0.997996  ...  0.710921  0.750296  0.761920   \n","2020  0.552764  0.613014  0.919840  ...  0.521411  0.700220  0.779435   \n","2020  0.351759  0.965753  0.861723  ...  0.672705  0.721347  0.694920   \n","2020  0.366834  0.851027  0.901804  ...  0.766624  0.868461  0.843694   \n","2021  0.547739  0.976027  1.000000  ...  1.000000  1.000000  0.852459   \n","\n","        X17_L1    X17_L2    X17_L3    X18_L1    X18_L2    X18_L3         Y  \n","YEAR                                                                        \n","2002  0.000000  0.041160  0.010611  0.000000  0.030742  0.004150  1.252763  \n","2002  0.037862  0.000000  0.000000  0.021448  0.000000  0.000000  1.029619  \n","2002  0.026720  0.069951  0.040340  0.019774  0.060789  0.031944  1.098612  \n","2002  0.049830  0.072384  0.048083  0.054719  0.065748  0.041498  0.741937  \n","2003  0.065924  0.097222  0.087085  0.094057  0.098770  0.066300 -1.203973  \n","...        ...       ...       ...       ...       ...       ...       ...  \n","2020  0.852058  0.825629  0.802983  0.935865  0.824276  0.813163       NaN  \n","2020  0.859899  0.732563  0.691139  0.893283  0.726597  0.839220       NaN  \n","2020  0.678737  0.577960  0.575662  0.711864  0.662832  0.711253  1.163151  \n","2020  0.894873  0.703264  0.706911  0.801632  0.690103  0.736055  0.741937  \n","2021  0.961003  0.867194  0.766466  0.972170  0.818128  0.739626  0.993252  \n","\n","[77 rows x 49 columns]"],"text/html":["\n","  <div id=\"df-0461057c-252c-476f-ba0a-0600dbc76bda\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>X1_L2</th>\n","      <th>X1_L3</th>\n","      <th>X2_L2</th>\n","      <th>X2_L3</th>\n","      <th>X3_L2</th>\n","      <th>X3_L3</th>\n","      <th>X4_L2</th>\n","      <th>X4_L3</th>\n","      <th>X5_L2</th>\n","      <th>X5_L3</th>\n","      <th>...</th>\n","      <th>X16_L1</th>\n","      <th>X16_L2</th>\n","      <th>X16_L3</th>\n","      <th>X17_L1</th>\n","      <th>X17_L2</th>\n","      <th>X17_L3</th>\n","      <th>X18_L1</th>\n","      <th>X18_L2</th>\n","      <th>X18_L3</th>\n","      <th>Y</th>\n","    </tr>\n","    <tr>\n","      <th>YEAR</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2002</th>\n","      <td>0.102941</td>\n","      <td>0.000000</td>\n","      <td>0.584906</td>\n","      <td>0.48</td>\n","      <td>0.119093</td>\n","      <td>0.000000</td>\n","      <td>0.546875</td>\n","      <td>0.608040</td>\n","      <td>0.111301</td>\n","      <td>0.000000</td>\n","      <td>...</td>\n","      <td>0.067586</td>\n","      <td>0.033960</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.041160</td>\n","      <td>0.010611</td>\n","      <td>0.000000</td>\n","      <td>0.030742</td>\n","      <td>0.004150</td>\n","      <td>1.252763</td>\n","    </tr>\n","    <tr>\n","      <th>2002</th>\n","      <td>0.779412</td>\n","      <td>0.592105</td>\n","      <td>0.679245</td>\n","      <td>0.54</td>\n","      <td>0.000000</td>\n","      <td>0.012397</td>\n","      <td>0.692708</td>\n","      <td>0.653266</td>\n","      <td>0.059932</td>\n","      <td>0.028056</td>\n","      <td>...</td>\n","      <td>0.153942</td>\n","      <td>0.121315</td>\n","      <td>0.103645</td>\n","      <td>0.037862</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.021448</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>1.029619</td>\n","    </tr>\n","    <tr>\n","      <th>2002</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.849057</td>\n","      <td>0.86</td>\n","      <td>0.177694</td>\n","      <td>0.090909</td>\n","      <td>0.776042</td>\n","      <td>0.844221</td>\n","      <td>0.176370</td>\n","      <td>0.114228</td>\n","      <td>...</td>\n","      <td>0.088554</td>\n","      <td>0.109613</td>\n","      <td>0.150115</td>\n","      <td>0.026720</td>\n","      <td>0.069951</td>\n","      <td>0.040340</td>\n","      <td>0.019774</td>\n","      <td>0.060789</td>\n","      <td>0.031944</td>\n","      <td>1.098612</td>\n","    </tr>\n","    <tr>\n","      <th>2002</th>\n","      <td>0.676471</td>\n","      <td>0.736842</td>\n","      <td>0.754717</td>\n","      <td>0.74</td>\n","      <td>0.119093</td>\n","      <td>0.090909</td>\n","      <td>0.854167</td>\n","      <td>0.783920</td>\n","      <td>0.111301</td>\n","      <td>0.080160</td>\n","      <td>...</td>\n","      <td>0.047361</td>\n","      <td>0.079850</td>\n","      <td>0.088818</td>\n","      <td>0.049830</td>\n","      <td>0.072384</td>\n","      <td>0.048083</td>\n","      <td>0.054719</td>\n","      <td>0.065748</td>\n","      <td>0.041498</td>\n","      <td>0.741937</td>\n","    </tr>\n","    <tr>\n","      <th>2003</th>\n","      <td>0.455882</td>\n","      <td>0.486842</td>\n","      <td>0.849057</td>\n","      <td>0.78</td>\n","      <td>0.206049</td>\n","      <td>0.138430</td>\n","      <td>0.723958</td>\n","      <td>0.763819</td>\n","      <td>0.188356</td>\n","      <td>0.146293</td>\n","      <td>...</td>\n","      <td>0.039290</td>\n","      <td>0.074096</td>\n","      <td>0.059700</td>\n","      <td>0.065924</td>\n","      <td>0.097222</td>\n","      <td>0.087085</td>\n","      <td>0.094057</td>\n","      <td>0.098770</td>\n","      <td>0.066300</td>\n","      <td>-1.203973</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>0.220588</td>\n","      <td>0.223684</td>\n","      <td>0.603774</td>\n","      <td>0.60</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.468750</td>\n","      <td>0.562814</td>\n","      <td>1.000000</td>\n","      <td>0.997996</td>\n","      <td>...</td>\n","      <td>0.710921</td>\n","      <td>0.750296</td>\n","      <td>0.761920</td>\n","      <td>0.852058</td>\n","      <td>0.825629</td>\n","      <td>0.802983</td>\n","      <td>0.935865</td>\n","      <td>0.824276</td>\n","      <td>0.813163</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>0.264706</td>\n","      <td>0.302632</td>\n","      <td>0.584906</td>\n","      <td>0.68</td>\n","      <td>0.809074</td>\n","      <td>0.882231</td>\n","      <td>0.328125</td>\n","      <td>0.552764</td>\n","      <td>0.613014</td>\n","      <td>0.919840</td>\n","      <td>...</td>\n","      <td>0.521411</td>\n","      <td>0.700220</td>\n","      <td>0.779435</td>\n","      <td>0.859899</td>\n","      <td>0.732563</td>\n","      <td>0.691139</td>\n","      <td>0.893283</td>\n","      <td>0.726597</td>\n","      <td>0.839220</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>0.102941</td>\n","      <td>0.197368</td>\n","      <td>0.000000</td>\n","      <td>0.08</td>\n","      <td>0.848771</td>\n","      <td>0.838843</td>\n","      <td>0.000000</td>\n","      <td>0.351759</td>\n","      <td>0.965753</td>\n","      <td>0.861723</td>\n","      <td>...</td>\n","      <td>0.672705</td>\n","      <td>0.721347</td>\n","      <td>0.694920</td>\n","      <td>0.678737</td>\n","      <td>0.577960</td>\n","      <td>0.575662</td>\n","      <td>0.711864</td>\n","      <td>0.662832</td>\n","      <td>0.711253</td>\n","      <td>1.163151</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>0.250000</td>\n","      <td>0.236842</td>\n","      <td>0.188679</td>\n","      <td>0.00</td>\n","      <td>0.839319</td>\n","      <td>0.929752</td>\n","      <td>0.359375</td>\n","      <td>0.366834</td>\n","      <td>0.851027</td>\n","      <td>0.901804</td>\n","      <td>...</td>\n","      <td>0.766624</td>\n","      <td>0.868461</td>\n","      <td>0.843694</td>\n","      <td>0.894873</td>\n","      <td>0.703264</td>\n","      <td>0.706911</td>\n","      <td>0.801632</td>\n","      <td>0.690103</td>\n","      <td>0.736055</td>\n","      <td>0.741937</td>\n","    </tr>\n","    <tr>\n","      <th>2021</th>\n","      <td>0.411765</td>\n","      <td>0.368421</td>\n","      <td>0.452830</td>\n","      <td>0.30</td>\n","      <td>0.986767</td>\n","      <td>0.933884</td>\n","      <td>0.510417</td>\n","      <td>0.547739</td>\n","      <td>0.976027</td>\n","      <td>1.000000</td>\n","      <td>...</td>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>0.852459</td>\n","      <td>0.961003</td>\n","      <td>0.867194</td>\n","      <td>0.766466</td>\n","      <td>0.972170</td>\n","      <td>0.818128</td>\n","      <td>0.739626</td>\n","      <td>0.993252</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>77 rows × 49 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0461057c-252c-476f-ba0a-0600dbc76bda')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-0461057c-252c-476f-ba0a-0600dbc76bda button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-0461057c-252c-476f-ba0a-0600dbc76bda');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["df[\"Y\"] = np.log1p(data[\"Y\"])\n"],"metadata":{"id":"F4hIIbHPYKGa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# X = train.drop(['Y'], axis=1)\n","# y = train['Y']\n","\n","import os "],"metadata":{"id":"E4oulUR_1k_p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everthing(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","  tf.random.set_seed(seed)\n","\n","seed_everthing(1)"],"metadata":{"id":"JRONeSZe3VZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rnn_model(X_train, y_train, X_test, y_test):\n","  model = Sequential()\n","\n","  # model.add(Embedding(input_dim =X_train.shape[1], output_dim = 1))\n","  model.add(Embedding(input_dim =X_train.shape[1], output_dim = 1))\n","\n","  # RNN 첫번째 은닉층 추가\n","  # model.add(LSTM(units=X_train.shape[1], activation='tanh', return_sequences = True))\n","  model.add(LSTM(units = 234, activation='tanh', return_sequences = True))\n","  # model.add(Dropout(0.6)) # 노드간의 커넥트를 랜덤하게 끊어줌\n","  model.add(Dropout(0.4))\n","\n","  # RNN 두번째 은닉층 추가\n","  model.add(LSTM(int(234/2), activation='tanh', return_sequences = True))\n","  model.add(Dropout(0.4))\n","\n","  # RNN 세번째 은닉층 추가\n","  model.add(LSTM(int(234/3), activation='tanh', return_sequences = True))\n","  model.add(Dropout(0.4))\n","\n","  # RNN 네번째 은닉층 추가\n","  model.add(LSTM(10, activation='tanh', return_sequences = False))\n","  model.add(Dropout(0.4))\n","\n","  # 출력층 추가\n","  model.add(Dense(units = 1))\n","\n","  model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","  checkpoint = ModelCheckpoint(filepath = f\"./model_{i}.h5\", monitor='val_loss', save_best_only=True)\n","  early_stopping = EarlyStopping(monitor='val_loss', patience =60, mode='min')\n","\n","  initial_learning_rate = 1e-4\n","  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps = 100, decay_rate = 0.96, staircase=True)\n","  scheduler = keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n","\n","  hist = model.fit(X_train, y_train, validation_data =(X_test, y_test), epochs=150,  batch_size = 8 ,callbacks=[checkpoint, early_stopping, scheduler])\n","  model_file = [fl for fl in os.listdir(\"/content/drive/MyDrive/hdf\") if re.findall(f\"model_{i}\", fl)][0]\n","  model = load_model(f\"/content/drive/MyDrive/hdf/{model_file}\")\n","\n","  return model"],"metadata":{"id":"ol5htmpd4DSJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split"],"metadata":{"id":"UGILN5LXzGS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.backend import rnn\n","sample_size = len(df)\n","predicted_y = []\n","\n","df_train = df[df.index < 2015] # 학습데이터\n","window = len(df_train) # 윈도우 사이즈 설정 (2002~2015)\n","\n","\n","df_test = df[df.index >= 2015] #검증데이터\n","forecast_horizon = len(df_test) # 검증데이터 길이\n","\n","start_index = 0\n","\n","# fit the model and make a forecast\n","for i in tqdm(range(forecast_horizon)): #검증데이터만큼 돌린다\n","\n","    # writer = SummaryWriter()\n","\n","    end_index = start_index + window \n","    new_data = df.iloc[start_index : end_index] # df에서 end_index+1\n","\n","    x_train, x_val, y_train, y_val = train_test_split(new_data.drop([\"Y\"], axis =1), new_data[\"Y\"] ,test_size = 0.25, shuffle = False)\n","\n","    # gets the trained neural network with given data\n","    model = rnn_model(x_train, y_train, x_val, y_val) #rnn_moel함수는 hist를 return해줌\n","\n","    x_new = df.iloc[end_index]\n","    x_new = pd.DataFrame(x_new).T.drop([\"Y\"], axis = 1)\n","    y_pred = model.predict(x_new)\n","    # y_pred = y_pred[0][0]    \n","\n","    predicted_y.append(y_pred)\n","\n","    start_index = start_index + 1 # for rolling-window forecasts\n","\n","    # window = window + 1 # for recursive forecasts\n","   "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9-IcQawi5TDs","executionInfo":{"status":"ok","timestamp":1654344106469,"user_tz":-540,"elapsed":2095238,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"90901eaa-f627-458e-bb22-36cfb9f8ddb0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/25 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 613ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 192ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 195ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 195ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r  4%|▍         | 1/25 [01:14<29:43, 74.32s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 621ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 215ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 2s 339ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 2s 331ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 225ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r  8%|▊         | 2/25 [02:46<32:32, 84.90s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 624ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 12%|█▏        | 3/25 [04:19<32:27, 88.50s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 618ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 193ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 274ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 2s 324ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 2s 331ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 16%|█▌        | 4/25 [05:52<31:35, 90.28s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 800ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe38bb66200> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stderr","text":["\r 20%|██        | 5/25 [07:24<30:18, 90.93s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 634ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 2s 341ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 2s 345ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 229ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fe38b750710> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"]},{"output_type":"stream","name":"stderr","text":["\r 24%|██▍       | 6/25 [08:57<29:00, 91.60s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 636ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 28%|██▊       | 7/25 [10:11<25:46, 85.92s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 620ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 214ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 32%|███▏      | 8/25 [11:44<24:58, 88.18s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 779ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 36%|███▌      | 9/25 [13:16<23:50, 89.41s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 631ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 194ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 40%|████      | 10/25 [14:31<21:12, 84.82s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 638ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 288ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 2s 341ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 2s 316ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 44%|████▍     | 11/25 [15:47<19:10, 82.18s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 623ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 48%|████▊     | 12/25 [17:20<18:31, 85.51s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 635ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 227ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 2s 337ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 2s 332ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 230ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 52%|█████▏    | 13/25 [18:53<17:33, 87.77s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 622ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 56%|█████▌    | 14/25 [20:26<16:22, 89.30s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 628ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 215ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 60%|██████    | 15/25 [21:41<14:09, 84.94s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 885ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 273ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 194ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 196ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 197ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 64%|██████▍   | 16/25 [22:55<12:16, 81.85s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 636ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 68%|██████▊   | 17/25 [24:10<10:37, 79.70s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 636ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 2s 341ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 2s 330ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 252ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 72%|███████▏  | 18/25 [25:28<09:13, 79.06s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 633ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 199ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 198ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 76%|███████▌  | 19/25 [26:42<07:45, 77.60s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 643ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 214ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 215ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 80%|████████  | 20/25 [28:15<06:51, 82.23s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 635ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 326ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 2s 341ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 294ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 84%|████████▍ | 21/25 [29:32<05:22, 80.74s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 818ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 88%|████████▊ | 22/25 [31:04<04:12, 84.16s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 640ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 214ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 214ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 323ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 2s 354ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 286ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 92%|█████████▏| 23/25 [32:22<02:44, 82.19s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 11s 648ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 216ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 200ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 202ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 201ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["\r 96%|█████████▌| 24/25 [33:38<01:20, 80.26s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Epoch 1: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 1/150\n","5/5 [==============================] - 10s 700ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 2: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 2/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 3: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 3/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 4: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 4/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 5: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 5/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 6: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 6/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 7: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 7/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 8: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 8/150\n","5/5 [==============================] - 1s 206ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 9: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 9/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 10: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 10/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 11: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 11/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 12: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 12/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 13: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 13/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 14: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 14/150\n","5/5 [==============================] - 1s 219ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 15: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 15/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 16: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 16/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 17: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 17/150\n","5/5 [==============================] - 1s 207ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 18: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 18/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 19: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 19/150\n","5/5 [==============================] - 1s 214ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 20: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 20/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 21: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 21/150\n","5/5 [==============================] - 1s 214ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 22: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 22/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 23: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 23/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 24: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 24/150\n","5/5 [==============================] - 1s 205ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 25: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 25/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 26: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 26/150\n","5/5 [==============================] - 1s 203ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 27: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 27/150\n","5/5 [==============================] - 1s 204ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 28: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 28/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 29: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 29/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 30: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 30/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 31: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 31/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 32: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 32/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 33: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 33/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 34: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 34/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 35: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 35/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 36: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 36/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 37: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 37/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 38: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 38/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 39: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 39/150\n","5/5 [==============================] - 1s 215ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 40: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 40/150\n","5/5 [==============================] - 1s 216ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 41: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 41/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 42: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 42/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 43: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 43/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 44: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 44/150\n","5/5 [==============================] - 1s 213ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 45: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 45/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 46: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 46/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 47: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 47/150\n","5/5 [==============================] - 1s 214ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 48: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 48/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 49: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 49/150\n","5/5 [==============================] - 1s 209ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 50: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 50/150\n","5/5 [==============================] - 1s 219ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 51: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 51/150\n","5/5 [==============================] - 1s 215ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 52: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 52/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 53: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 53/150\n","5/5 [==============================] - 1s 212ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 54: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 54/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 55: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 55/150\n","5/5 [==============================] - 1s 210ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 56: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 56/150\n","5/5 [==============================] - 1s 215ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 57: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 57/150\n","5/5 [==============================] - 1s 211ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 58: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 58/150\n","5/5 [==============================] - 1s 208ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 59: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 59/150\n","5/5 [==============================] - 1s 215ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n","\n","Epoch 60: LearningRateScheduler setting learning rate to 9.999999747378752e-05.\n","Epoch 60/150\n","5/5 [==============================] - 1s 218ms/step - loss: nan - root_mean_squared_error: nan - val_loss: nan - val_root_mean_squared_error: nan - lr: 1.0000e-04\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 25/25 [34:55<00:00, 83.80s/it]\n"]}]},{"cell_type":"code","source":["predicted_y"],"metadata":{"id":"m-KDWYZrV1c1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"F53ji9F3s_Yb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["predicted_y = [v[0][0] for v in predicted_y]"],"metadata":{"id":"aP9KUbdhCzrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test[\"loss\"] = predicted_y - df_test[\"Y\"]\n","df_test[\"predicted_y\"] = predicted_y"],"metadata":{"id":"25I-fT_-KDm8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test[\"Y_exp\"] = np.exp(df_test[\"Y\"])\n","df_test[\"predicted_y_exp\"] = np.exp(df_test[\"predicted_y\"])\n","# df_test[\"predicted_y_r\"] = round(df_test[\"predicted_y\"], 1)"],"metadata":{"id":"eCHp9w4-7yya"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_test[\"Y\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GSm9hVHUvnkg","executionInfo":{"status":"ok","timestamp":1654346650216,"user_tz":-540,"elapsed":355,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"cebff916-660f-43d9-c090-d00fa60d9d35"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["YEAR\n","2015    0.587787\n","2015    0.336472\n","2015    0.916291\n","2015    0.530628\n","2016    0.262364\n","2016    0.741937\n","2016    0.405465\n","2016    0.470004\n","2017    0.693147\n","2017    0.530628\n","2017    0.916291\n","2017   -0.356675\n","2018    0.741937\n","2018    0.470004\n","2018    0.530628\n","2018    0.587787\n","2019   -0.223144\n","2019    0.693147\n","2019    0.336472\n","2019    0.832909\n","2020         NaN\n","2020         NaN\n","2020    1.163151\n","2020    0.741937\n","2021    0.993252\n","Name: Y, dtype: float64"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["df_test[[\"Y_exp\", \"predicted_y_exp\", \"loss\"]]#.to_csv(\"./prediction_result.csv\", sep = \",\", encoding = \"cp949\")"],"metadata":{"id":"iB6zWkSSKXqF","executionInfo":{"status":"ok","timestamp":1654347435403,"user_tz":-540,"elapsed":293,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"colab":{"base_uri":"https://localhost:8080/","height":864},"outputId":"339a26dc-3c45-4791-d4b4-48b6f1d12da8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      Y_exp  predicted_y_exp      loss\n","YEAR                                  \n","2015    1.8         1.883491  0.045340\n","2015    1.4         1.895097  0.302798\n","2015    2.5         1.890210 -0.279603\n","2015    1.7         1.905734  0.114239\n","2016    1.3         1.911419  0.385481\n","2016    2.1         1.904399 -0.097771\n","2016    1.5         1.907272  0.240209\n","2016    1.6         1.893813  0.168589\n","2017    2.0         1.886347 -0.058505\n","2017    1.7         1.891133  0.106548\n","2017    2.5         1.889529 -0.279963\n","2017    0.7         1.903996  1.000630\n","2018    2.1         1.888985 -0.105897\n","2018    1.6         1.906801  0.175423\n","2018    1.7         1.899426  0.110923\n","2018    1.8         1.908253  0.058401\n","2019    0.8         1.889118  0.859253\n","2019    2.0         1.873856 -0.065149\n","2019    1.4         1.887000  0.298516\n","2019    2.3         1.874649 -0.204487\n","2020    NaN         1.888596       NaN\n","2020    NaN         1.850852       NaN\n","2020    3.2         1.758400 -0.598747\n","2020    2.1         1.785513 -0.162231\n","2021    2.7         1.782594 -0.415182"],"text/html":["\n","  <div id=\"df-c3a1fc3e-a528-4eac-8968-f07729036c60\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Y_exp</th>\n","      <th>predicted_y_exp</th>\n","      <th>loss</th>\n","    </tr>\n","    <tr>\n","      <th>YEAR</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2015</th>\n","      <td>1.8</td>\n","      <td>1.883491</td>\n","      <td>0.045340</td>\n","    </tr>\n","    <tr>\n","      <th>2015</th>\n","      <td>1.4</td>\n","      <td>1.895097</td>\n","      <td>0.302798</td>\n","    </tr>\n","    <tr>\n","      <th>2015</th>\n","      <td>2.5</td>\n","      <td>1.890210</td>\n","      <td>-0.279603</td>\n","    </tr>\n","    <tr>\n","      <th>2015</th>\n","      <td>1.7</td>\n","      <td>1.905734</td>\n","      <td>0.114239</td>\n","    </tr>\n","    <tr>\n","      <th>2016</th>\n","      <td>1.3</td>\n","      <td>1.911419</td>\n","      <td>0.385481</td>\n","    </tr>\n","    <tr>\n","      <th>2016</th>\n","      <td>2.1</td>\n","      <td>1.904399</td>\n","      <td>-0.097771</td>\n","    </tr>\n","    <tr>\n","      <th>2016</th>\n","      <td>1.5</td>\n","      <td>1.907272</td>\n","      <td>0.240209</td>\n","    </tr>\n","    <tr>\n","      <th>2016</th>\n","      <td>1.6</td>\n","      <td>1.893813</td>\n","      <td>0.168589</td>\n","    </tr>\n","    <tr>\n","      <th>2017</th>\n","      <td>2.0</td>\n","      <td>1.886347</td>\n","      <td>-0.058505</td>\n","    </tr>\n","    <tr>\n","      <th>2017</th>\n","      <td>1.7</td>\n","      <td>1.891133</td>\n","      <td>0.106548</td>\n","    </tr>\n","    <tr>\n","      <th>2017</th>\n","      <td>2.5</td>\n","      <td>1.889529</td>\n","      <td>-0.279963</td>\n","    </tr>\n","    <tr>\n","      <th>2017</th>\n","      <td>0.7</td>\n","      <td>1.903996</td>\n","      <td>1.000630</td>\n","    </tr>\n","    <tr>\n","      <th>2018</th>\n","      <td>2.1</td>\n","      <td>1.888985</td>\n","      <td>-0.105897</td>\n","    </tr>\n","    <tr>\n","      <th>2018</th>\n","      <td>1.6</td>\n","      <td>1.906801</td>\n","      <td>0.175423</td>\n","    </tr>\n","    <tr>\n","      <th>2018</th>\n","      <td>1.7</td>\n","      <td>1.899426</td>\n","      <td>0.110923</td>\n","    </tr>\n","    <tr>\n","      <th>2018</th>\n","      <td>1.8</td>\n","      <td>1.908253</td>\n","      <td>0.058401</td>\n","    </tr>\n","    <tr>\n","      <th>2019</th>\n","      <td>0.8</td>\n","      <td>1.889118</td>\n","      <td>0.859253</td>\n","    </tr>\n","    <tr>\n","      <th>2019</th>\n","      <td>2.0</td>\n","      <td>1.873856</td>\n","      <td>-0.065149</td>\n","    </tr>\n","    <tr>\n","      <th>2019</th>\n","      <td>1.4</td>\n","      <td>1.887000</td>\n","      <td>0.298516</td>\n","    </tr>\n","    <tr>\n","      <th>2019</th>\n","      <td>2.3</td>\n","      <td>1.874649</td>\n","      <td>-0.204487</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>NaN</td>\n","      <td>1.888596</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>NaN</td>\n","      <td>1.850852</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>3.2</td>\n","      <td>1.758400</td>\n","      <td>-0.598747</td>\n","    </tr>\n","    <tr>\n","      <th>2020</th>\n","      <td>2.1</td>\n","      <td>1.785513</td>\n","      <td>-0.162231</td>\n","    </tr>\n","    <tr>\n","      <th>2021</th>\n","      <td>2.7</td>\n","      <td>1.782594</td>\n","      <td>-0.415182</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c3a1fc3e-a528-4eac-8968-f07729036c60')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c3a1fc3e-a528-4eac-8968-f07729036c60 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c3a1fc3e-a528-4eac-8968-f07729036c60');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["print((np.sum((df_test['predicted_y_exp'] - df_test[\"Y_exp\"]) ** 2) / len(df_test['predicted_y_exp'])) ** 1/2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fo_uoJ8w02SW","executionInfo":{"status":"ok","timestamp":1654347546488,"user_tz":-540,"elapsed":4,"user":{"displayName":"김다은","userId":"16190703000834159129"}},"outputId":"6cf7e91e-5b8c-4170-d54c-6cc84da3d750"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.16062710583309672\n"]}]},{"cell_type":"code","source":[" y_vloss = hist.history['loss']\n","y_loss = hist.history['val_loss']\n","\n","x_len = np.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"],"metadata":{"id":"JEKQE0VG4DVl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.sqr(predicted_y_list)"],"metadata":{"id":"OeEA4D5RisOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 교차검증 시 데이터 분할 시각화 helper function 가져오기\n","from sklearn.model_selection import TimeSeriesSplit \n","n_split = 4\n","tscv = TimeSeriesSplit(n_splits=n_split)\n","X = X.set_index(data[\"datetime\"])\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n","  print(\"Fold: {}\".format(fold))\n","  print(\"TRAIN indices:\", train_index, \"\\n\", \"TEST indices:\", test_index)\n","  print(\"\\n\")\n","  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","  y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","  "],"metadata":{"id":"GU8yLIzx1lDB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"y6KTxbfIisRr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### covid 미포함"],"metadata":{"id":"Pyk6tqkI9ZY4"}},{"cell_type":"code","source":["data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/gdp_re/training_data_0524.csv')\n","data = data.drop(['Unnamed: 0'], axis=1)\n","data.tail()"],"metadata":{"id":"63bjvqs19ZY5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data[data.iloc[:, 9:].apply(lambda row : sum(pd.isna(row)), axis = 1) == 0]\n","data = data.reset_index(drop = True)\n","data"],"metadata":{"id":"-h3v4yJp9ZY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data[(data['YEAR']!=2020)&(data['YEAR']!=2021)]\n","data"],"metadata":{"id":"gBPZ6tbS91Rp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['TIME'] = data['TIME'].astype(str)\n","\n","time_list = []\n","for i in range(len(data)):\n","  time_i = data['TIME'][i]\n","  time = re.sub(r'(\\d{4})(\\d{2})', '\\g<1>-\\g<2>', time_i) #정규표현식 'YYYY-mm'\n","  time_list.append(time)\n","      \n","data['TIME'] = time_list\n","data['datetime'] = pd.to_datetime(data['TIME'],  format=\"%Y-%m\") #datetime변환\n","data.sort_values('datetime', ignore_index = True, inplace=True)"],"metadata":{"id":"EwdZdhBu9ZY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.tail()"],"metadata":{"id":"w6iJmsPK9ZY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = data.drop(['Y1','Y2','Y3','YY1','YY2','YY3','MONTH','YEAR','QRT','TIME','STATE','datetime'], axis=1)\n","train.columns\n","## 다 떄려넣기\n","## 성능확인\n","## 너무 다른얟,림ㅇ"],"metadata":{"id":"bAoyr2q89ZY6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ##정규화\n","# data['ride_passen_log'] = np.log1p(data['ride_passen'])\n","# data['takeoff_passen_log'] = np.log1p(data['takeoff_passen'])\n","# data.head(13)\n","\n","## 표준화\n","from sklearn.preprocessing import StandardScaler\n","# train = train.drop(['MONTH', 'YEAR', 'QRT', 'TIME', 'STATE', 'Y1',\t'Y2',\t'Y3',\t'YY1',\t'YY2',\t'YY3', 'datetime'], axis=1)\n","train = StandardScaler().fit_transform(train)\n","train"],"metadata":{"id":"2Os0NA859ZY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ## 정규화\n","scaler = MinMaxScaler()\n","scaler.fit(train)\n","train = scaler.transform(train)\n","train"],"metadata":{"id":"BiiYHV0_9ZY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train = pd.DataFrame(train, columns=[['Y', 'X1_L2', 'X1_L3', 'X2_L2', 'X2_L3', 'X3_L2', 'X3_L3', 'X4_L2',\n","       'X4_L3', 'X5_L2', 'X5_L3', 'X6_L2', 'X6_L3', 'X7_L2', 'X7_L3', 'X8_L1',\n","       'X8_L2', 'X8_L3', 'X9_L1', 'X9_L2', 'X9_L3', 'X10_L1', 'X10_L2',\n","       'X10_L3', 'X11_L1', 'X11_L2', 'X11_L3', 'X12_L2', 'X12_L3', 'X13_L2',\n","       'X13_L3', 'X14_L0', 'X14_L1', 'X14_L2', 'X14_L3', 'X15_L0', 'X15_L1',\n","       'X15_L2', 'X15_L3', 'X16_L0', 'X16_L1', 'X16_L2', 'X16_L3', 'X17_L1',\n","       'X17_L2', 'X17_L3', 'X18_L1', 'X18_L2', 'X18_L3']])\n","train"],"metadata":{"id":"p3BJK8Wl9ZY7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = train.drop(['Y'], axis=1)\n","y = train['Y']"],"metadata":{"id":"H4IO23uy9ZY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## 교차검증 시 데이터 분할 시각화 helper function 가져오기\n","from sklearn.model_selection import TimeSeriesSplit \n","n_split = 4\n","tscv = TimeSeriesSplit(n_splits=n_split)\n","X = X.set_index(data[\"datetime\"])\n","\n","for fold, (train_index, test_index) in enumerate(tscv.split(X)):\n","  print(\"Fold: {}\".format(fold))\n","  print(\"TRAIN indices:\", train_index, \"\\n\", \"TEST indices:\", test_index)\n","  print(\"\\n\")\n","  X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","  y_train, y_test = y.iloc[train_index], y.iloc[test_index]"],"metadata":{"id":"QGBku8iO9ZY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def seed_everthing(seed):\n","  random.seed(seed)\n","  np.random.seed(seed)\n","  os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","  tf.random.set_seed(seed)\n","\n","seed_everthing(1)"],"metadata":{"id":"fYg9Vros9ZY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train"],"metadata":{"id":"TjoGOpH69ZY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Sequential()\n","\n","# model.add(keras.Input(shape=(X_train.shape[1],1)))\n","# model.add(Dense(self.units, activation='tanh')\n","# model.layers.Embedding(67, 48, input_length = 1)\n","# model.add(layers.Embedding(input_dim = 67, output_dim = 1))\n","model.add(Embedding(input_dim =X_train.shape[1], output_dim = 1))\n","\n","# RNN 첫번째 은닉층 추가\n","model.add(SimpleRNN(units=X_train.shape[1], activation='tanh', return_sequences = True))\n","model.add(Dropout(0.6)) # 노드간의 커넥트를 랜덤하게 끊어줌\n","\n","# RNN 두번째 은닉층 추가\n","model.add(SimpleRNN(int(X_train.shape[1]/2), activation='tanh', return_sequences = True)) # True : many to many, 혹은 여러 RNN / LSTM 층을 쌓을때 사용, False = many to one, Dense 층으로 넘겨줄때\n","model.add(Dropout(0.5))\n","\n","# RNN 세번째 은닉층 추가\n","model.add(SimpleRNN(int(X_train.shape[1]/3), activation='tanh', return_sequences = True)) # True : many to many, 혹은 여러 RNN / LSTM 층을 쌓을때 사용, False = many to one, Dense 층으로 넘겨줄때\n","model.add(Dropout(0.3))\n","\n","# # RNN 네번째 은닉층 추가\n","# model.add(SimpleRNN(int(self.units/4), activation='tanh', return_sequences = True)) # True : many to many, 혹은 여러 RNN / LSTM 층을 쌓을때 사용, False = many to one, Dense 층으로 넘겨줄때\n","# model.add(Dropout(0.3))\n","\n","model.add(SimpleRNN(10, activation='tanh', return_sequences = False)) # True : many to many, 혹은 여러 RNN / LSTM 층을 쌓을때 사용, False = many to one, Dense 층으로 넘겨줄때\n","model.add(Dropout(0.3))\n","\n","# # RNN 세번째 은닉층 추가\n","# # model.add(SimpleRNN(self.units, activation='tanh', return_sequences=True))\n","# model.add(Dense(units = self.units / 4, activation=\"ReLU\"))\n","# # model.add(SimpleRNN(units = 10, activation='tanh', return_sequences=True))\n","# model.add(Dropout(0.2))\n","\n","# 출력층 추가: 분류모델 제외하고 활성화함수 사용X\n","model.add(Dense(units = 1)) # 다대일\n","\n","## [3] compile: RNN모델을 학습시키기 위한 학습과정 설정하는 단계\n","model.compile(optimizer='adam', loss='mean_squared_error', metrics=[tf.keras.metrics.RootMeanSquaredError()])\n","\n","## [4] 학습데이터로 RNN모델 학습\n","# earlystopping 선언\n","save_folder = '/content/drive/MyDrive/Colab Notebooks/교수님/hdf5/'\n","model_path = save_folder + 'mnist-' + '{epoch:02d}-{val_loss:.4f}.hdf5'\n","\n","checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', save_best_only=True)\n","early_stopping = EarlyStopping(monitor='val_loss', patience =60, mode='min')\n","\n","initial_learning_rate = 0.01\n","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps = 100, decay_rate = 0.96, staircase=True)\n","scheduler = keras.callbacks.LearningRateScheduler(lr_schedule, verbose=1)\n","\n","hist = model.fit(X_train, y_train, validation_data =(X_test, y_test), epochs=100,  batch_size =16 ,callbacks=[checkpoint, early_stopping, scheduler])"],"metadata":{"id":"5G88CWBZ9ZY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_vloss = hist.history['loss']\n","y_loss = hist.history['val_loss']\n","\n","x_len = np.arange(len(y_loss))\n","plt.plot(x_len, y_vloss, marker='.', c='red', label=\"Validation-set Loss\")\n","plt.plot(x_len, y_loss, marker='.', c='blue', label=\"Train-set Loss\")\n","\n","plt.legend(loc='upper right')\n","plt.grid()\n","plt.xlabel('epoch')\n","plt.ylabel('loss')\n","plt.show()"],"metadata":{"id":"_11XJ83K9ZY8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"tjfh0INu9ZY9"},"execution_count":null,"outputs":[]}]}